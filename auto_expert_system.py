#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿ (Auto Expert Selection System)

è¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åˆ†ææ•°æ®é›†åˆ†å¸ƒï¼Œè¯†åˆ«å°‘æ•°ç±»ï¼Œè®¾è®¡æœ€ä¼˜ä¸“å®¶ç­–ç•¥ï¼Œ
å¹¶å®Œæˆç«¯åˆ°ç«¯çš„è®­ç»ƒä¸è¯„ä¼°æµç¨‹ã€‚

ä½œè€…: Deep-Packet Research Team
æ—¥æœŸ: 2025å¹´11æœˆ7æ—¥
"""

import os
import sys
import json
import subprocess
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, ArrayType, LongType, DoubleType

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from utils import ID_TO_TRAFFIC

@dataclass
class ExpertConfig:
    """ä¸“å®¶é…ç½®æ•°æ®ç±»"""
    name: str
    target_classes: List[int]
    description: str
    dataset_name: str
    model_path: Optional[str] = None

@dataclass
class AnalysisResults:
    """æ•°æ®åˆ†æç»“æœ"""
    class_distribution: Dict[int, int]
    total_samples: int
    total_classes: int
    statistics: Dict[str, float]
    minority_classes: List[int]
    expert_strategy: Dict

class AutoExpertSystem:
    """è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿä¸»ç±»"""

    def __init__(self, data_source_dir: str, output_base_dir: str = "auto_expert_results"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–ä¸“å®¶ç³»ç»Ÿ

        Args:
            data_source_dir: é¢„å¤„ç†æ•°æ®æºç›®å½•
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.data_source_dir = data_source_dir
        self.output_base_dir = output_base_dir
        self.spark = None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_base_dir, exist_ok=True)

        # ç³»ç»Ÿé…ç½®
        self.config = {
            'minority_detection_strategies': ['magnitude', 'logarithmic', 'tier'],
            'magnitude_ratio': 5,  # æ•°é‡çº§å·®å¼‚å€æ•° (5å€å·®å¼‚)
            'log_std_threshold': 0.75,  # å¯¹æ•°æ ‡å‡†å·®é˜ˆå€¼ (é€‚ä¸­æ ‡å‡†)
            'tier_ratio': 3,  # æ¢¯é˜Ÿå·®å¼‚å€æ•° (3å€å·®å¼‚ï¼Œæ›´ä¸¥æ ¼)
            'max_minority_experts': 3,  # æœ€å¤§å°‘æ•°ç±»ä¸“å®¶æ•°
            'min_expert_samples': 50  # ä¸“å®¶è®­ç»ƒæœ€å°æ ·æœ¬æ•°
        }

    def _init_spark(self):
        """åˆå§‹åŒ–Sparkä¼šè¯"""
        if self.spark is None:
            os.environ["PYSPARK_PYTHON"] = sys.executable
            os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable
            self.spark = (
                SparkSession.builder.master("local[*]")
                .config("spark.driver.host", "127.0.0.1")
                .getOrCreate()
            )

    def analyze_class_distribution(self) -> Dict[int, int]:
        """
        åˆ†ææ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ

        Returns:
            ç±»åˆ«åˆ†å¸ƒå­—å…¸ {class_id: sample_count}
        """
        print("ğŸ” åˆ†ææ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ...")

        self._init_spark()

        schema = StructType([
            StructField("app_label", LongType(), True),
            StructField("traffic_label", LongType(), True),
            StructField("feature", ArrayType(DoubleType()), True),
        ])

        df = self.spark.read.schema(schema).json(f"{self.data_source_dir}/*.json.gz")

        # ç»Ÿè®¡traffic_labelåˆ†å¸ƒ
        traffic_label_counts = (df.filter(col("traffic_label").isNotNull())
                              .groupBy("traffic_label")
                              .count()
                              .orderBy("traffic_label")
                              .toPandas())

        # è½¬æ¢ä¸ºå­—å…¸
        class_distribution = dict(zip(traffic_label_counts['traffic_label'],
                                    traffic_label_counts['count']))

        print(f"âœ… å‘ç° {len(class_distribution)} ä¸ªç±»åˆ«ï¼Œæ€»è®¡ {sum(class_distribution.values())} ä¸ªæ ·æœ¬")

        # æ‰“å°è¯¦ç»†åˆ†å¸ƒ
        print("\nğŸ“Š ç±»åˆ«åˆ†å¸ƒè¯¦æƒ…:")
        for class_id, count in sorted(class_distribution.items()):
            class_name = ID_TO_TRAFFIC.get(class_id, f"Unknown-{class_id}")
            print(f"   ç±»åˆ« {class_id} ({class_name}): {count:,} æ ·æœ¬")

        return class_distribution

    def calculate_distribution_statistics(self, class_distribution: Dict[int, int]) -> Dict[str, float]:
        """
        è®¡ç®—åˆ†å¸ƒç»Ÿè®¡ç‰¹å¾

        Args:
            class_distribution: ç±»åˆ«åˆ†å¸ƒå­—å…¸

        Returns:
            ç»Ÿè®¡ç‰¹å¾å­—å…¸
        """
        counts = list(class_distribution.values())

        stats = {
            'mean': np.mean(counts),
            'std': np.std(counts),
            'median': np.median(counts),
            'min': np.min(counts),
            'max': np.max(counts),
            'q25': np.percentile(counts, 25),
            'q75': np.percentile(counts, 75),
            'iqr': np.percentile(counts, 75) - np.percentile(counts, 25),
            'cv': np.std(counts) / np.mean(counts) if np.mean(counts) > 0 else 0  # å˜å¼‚ç³»æ•°
        }

        print(f"\nğŸ“ˆ åˆ†å¸ƒç»Ÿè®¡ç‰¹å¾:")
        print(f"   å¹³å‡æ ·æœ¬æ•°: {stats['mean']:,.0f}")
        print(f"   ä¸­ä½æ•°: {stats['median']:,.0f}")
        print(f"   æ ‡å‡†å·®: {stats['std']:,.0f}")
        print(f"   æœ€å°å€¼: {stats['min']:,}")
        print(f"   æœ€å¤§å€¼: {stats['max']:,}")
        print(f"   å˜å¼‚ç³»æ•°: {stats['cv']:.3f}")

        return stats

    def identify_minority_classes(self, class_distribution: Dict[int, int],
                                stats: Dict[str, float]) -> List[int]:
        """
        åŸºäºé•¿å°¾åˆ†å¸ƒç‰¹å¾çš„å¤šç­–ç•¥å°‘æ•°ç±»è¯†åˆ«

        Args:
            class_distribution: ç±»åˆ«åˆ†å¸ƒ
            stats: ç»Ÿè®¡ç‰¹å¾ (ç”¨äºå…¼å®¹æ€§ï¼Œå®é™…ä¸ä½¿ç”¨)

        Returns:
            å°‘æ•°ç±»IDåˆ—è¡¨
        """
        print("\nğŸ¯ åŸºäºé•¿å°¾åˆ†å¸ƒç‰¹å¾çš„å¤šç­–ç•¥å°‘æ•°ç±»è¯†åˆ«...")

        minority_candidates = set()
        strategy_results = {}

        # ç­–ç•¥1: æ•°é‡çº§å·®å¼‚æ£€æµ‹ (5å€å·®å¼‚åŸåˆ™)
        if 'magnitude' in self.config['minority_detection_strategies']:
            max_count = max(class_distribution.values())
            magnitude_threshold = max_count / self.config['magnitude_ratio']  # 5å€å·®å¼‚
            magnitude_minority = [cls for cls, count in class_distribution.items()
                                 if count < magnitude_threshold]
            minority_candidates.update(magnitude_minority)
            strategy_results['magnitude'] = magnitude_minority
            print(f"   æ•°é‡çº§å·®å¼‚æ£€æµ‹ (<{magnitude_threshold:,.0f}, 1/{self.config['magnitude_ratio']}å€): {magnitude_minority}")

        # ç­–ç•¥2: å¯¹æ•°å°ºåº¦æ£€æµ‹ (åŸºäºå¯¹æ•°æ ‡å‡†å·®)
        if 'logarithmic' in self.config['minority_detection_strategies']:
            log_counts = np.log10(list(class_distribution.values()))
            log_mean = np.mean(log_counts)
            log_std = np.std(log_counts)
            log_threshold = log_mean - self.config['log_std_threshold']  # ä½äºå‡å€¼1ä¸ªæ ‡å‡†å·®
            log_minority = [cls for cls, count in class_distribution.items()
                           if np.log10(count) < log_threshold]
            minority_candidates.update(log_minority)
            strategy_results['logarithmic'] = log_minority
            print(f"   å¯¹æ•°å°ºåº¦æ£€æµ‹ (log<{log_threshold:.2f}): {log_minority}")

        # ç­–ç•¥3: æ¢¯é˜Ÿå·®å¼‚æ£€æµ‹ (ç›¸é‚»ç±»åˆ«3å€å·®å¼‚)
        if 'tier' in self.config['minority_detection_strategies']:
            tier_minority = self._detect_tier_gaps(class_distribution)
            minority_candidates.update(tier_minority)
            strategy_results['tier'] = tier_minority
            print(f"   æ¢¯é˜Ÿå·®å¼‚æ£€æµ‹ (>={self.config['tier_ratio']}å€å·®å¼‚): {tier_minority}")

        # ç»Ÿè®¡å„ç­–ç•¥çš„äº¤é›†å’Œå¹¶é›†
        all_strategies = list(strategy_results.values())
        if all_strategies:
            union = set().union(*all_strategies)

            # è®¡ç®—äº¤é›†ï¼ˆç”¨äºåˆ†æï¼‰
            if len(all_strategies) > 1:
                intersection = set(all_strategies[0]).intersection(*all_strategies[1:])
            else:
                intersection = set(all_strategies[0])
        else:
            union = set()
            intersection = set()

        print(f"\nğŸ“‹ ç­–ç•¥æ±‡æ€»:")
        print(f"   å„ç­–ç•¥äº¤é›†: {list(intersection)}")
        print(f"   å„ç­–ç•¥å¹¶é›†: {list(union)}")

        # å†³ç­–é€»è¾‘ï¼šè‡³å°‘è¢«2ä¸ªç­–ç•¥è¯†åˆ«çš„ç±»åˆ«æ‰è¢«è®¤ä¸ºæ˜¯çœŸæ­£çš„å°‘æ•°ç±»
        final_minority = []
        if union:
            for cls in union:
                vote_count = sum(1 for strategy_classes in strategy_results.values() if cls in strategy_classes)
                if vote_count >= 2:  # è‡³å°‘è¢«2ä¸ªç­–ç•¥è¯†åˆ«
                    final_minority.append(cls)

        # å¦‚æœæ²¡æœ‰ç±»åˆ«è¢«å¤šä¸ªç­–ç•¥è¯†åˆ«ï¼Œè¯´æ˜æ•°æ®é›†ä¸­æ²¡æœ‰æ˜æ˜¾çš„å°‘æ•°ç±»
        if not final_minority:
            print("   ğŸ“Œ ç»“è®º: æ²¡æœ‰ç±»åˆ«è¢«å¤šä¸ªç­–ç•¥ä¸€è‡´è¯†åˆ«ä¸ºå°‘æ•°ç±»")
            print("   ğŸ“Œ å»ºè®®: å¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥å‚æ•°æˆ–ä½¿ç”¨å•ä¸€åŸºå‡†æ¨¡å‹")
            return []

        # æŒ‰æ ·æœ¬æ•°é‡æ’åº
        final_minority.sort(key=lambda x: class_distribution[x])

        print(f"ğŸ‰ æœ€ç»ˆç¡®å®šçš„å°‘æ•°ç±»: {final_minority}")
        for cls in final_minority:
            class_name = ID_TO_TRAFFIC.get(cls, f"Unknown-{cls}")
            print(f"   ç±»åˆ« {cls} ({class_name}): {class_distribution[cls]:,} æ ·æœ¬")

            # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„æŠ•ç¥¨æƒ…å†µ
            votes = []
            for strategy_name, strategy_classes in strategy_results.items():
                if cls in strategy_classes:
                    votes.append(strategy_name)
            print(f"     æ”¯æŒç­–ç•¥: {', '.join(votes)}")

        return final_minority

    def _detect_tier_gaps(self, class_distribution: Dict[int, int]) -> List[int]:
        """
        æ£€æµ‹æ¢¯é˜Ÿå·®å¼‚ - è¯†åˆ«ç›¸é‚»ç±»åˆ«é—´çš„æ˜¾è‘—å·®å¼‚

        Args:
            class_distribution: ç±»åˆ«åˆ†å¸ƒå­—å…¸

        Returns:
            è¢«è¯†åˆ«ä¸ºå°‘æ•°ç±»çš„ç±»åˆ«åˆ—è¡¨
        """
        # æŒ‰æ ·æœ¬æ•°é‡æ’åº
        sorted_items = sorted(class_distribution.items(), key=lambda x: x[1])

        tier_minority = []

        # æ£€æŸ¥æ‰€æœ‰ç›¸é‚»ç±»åˆ«ä¹‹é—´çš„å·®å¼‚ï¼Œæ”¶é›†æ‰€æœ‰å¯èƒ½çš„å°‘æ•°ç±»
        for i in range(1, len(sorted_items)):
            prev_cls, prev_count = sorted_items[i-1]
            curr_cls, curr_count = sorted_items[i]

            # å¦‚æœå‘ç°æ˜¾è‘—å·®å¼‚
            if curr_count >= prev_count * self.config['tier_ratio']:
                # å‰é¢çš„ç±»åˆ«å¯èƒ½æ˜¯å°‘æ•°ç±»
                tier_minority.append(prev_cls)
                print(f"     å‘ç°æ¢¯é˜Ÿå·®å¼‚: ç±»åˆ«{prev_cls}({prev_count:,}) -> ç±»åˆ«{curr_cls}({curr_count:,})")

        # å¦‚æœæ²¡æœ‰å‘ç°æ¢¯é˜Ÿå·®å¼‚ï¼Œæ£€æŸ¥æœ€å°çš„ç±»åˆ«æ˜¯å¦æ˜æ˜¾å°äºå…¶ä»–ç±»åˆ«
        if not tier_minority and len(sorted_items) >= 2:
            smallest_cls, smallest_count = sorted_items[0]
            second_smallest_cls, second_smallest_count = sorted_items[1]

            # å¦‚æœæœ€å°ç±»åˆ«æ˜æ˜¾å°äºç¬¬äºŒå°ç±»åˆ«
            if second_smallest_count >= smallest_count * self.config['tier_ratio']:
                tier_minority.append(smallest_cls)
                print(f"     æœ€å°ç±»åˆ«æ˜¾è‘—è¾ƒå°: ç±»åˆ«{smallest_cls}({smallest_count:,})")

        return sorted(list(set(tier_minority)))  # å»é‡å¹¶æ’åº

    def design_expert_strategy(self, class_distribution: Dict[int, int],
                             minority_classes: List[int]) -> Dict:
        """
        è®¾è®¡ä¸“å®¶ç­–ç•¥

        Args:
            class_distribution: ç±»åˆ«åˆ†å¸ƒ
            minority_classes: å°‘æ•°ç±»åˆ—è¡¨

        Returns:
            ä¸“å®¶ç­–ç•¥é…ç½®
        """
        print("\nğŸ§  è®¾è®¡ä¸“å®¶ç­–ç•¥...")

        total_classes = len(class_distribution)
        minority_count = len(minority_classes)

        # ç­–ç•¥å†³ç­–æ ‘
        if minority_count == 0:
            print("   ğŸ“Œ ç­–ç•¥: æ— æ˜æ˜¾å°‘æ•°ç±»ï¼Œä½¿ç”¨å•ä¸€åŸºå‡†æ¨¡å‹")
            return {
                'type': 'single',
                'experts': [
                    ExpertConfig(
                        name='baseline',
                        target_classes=list(class_distribution.keys()),
                        description='åŸºå‡†æ¨¡å‹ï¼Œå¤„ç†æ‰€æœ‰ç±»åˆ«',
                        dataset_name='baseline_all_classes'
                    )
                ]
            }

        elif minority_count <= 2:
            print(f"   ğŸ“Œ ç­–ç•¥: å°‘é‡å°‘æ•°ç±»({minority_count}ä¸ª)ï¼Œä½¿ç”¨åŸºå‡†æ¨¡å‹ + å•ä¸€å°‘æ•°ç±»ä¸“å®¶")
            return {
                'type': 'baseline_plus_minority',
                'experts': [
                    ExpertConfig(
                        name='baseline',
                        target_classes=list(class_distribution.keys()),
                        description='åŸºå‡†æ¨¡å‹ï¼Œå¤„ç†æ‰€æœ‰ç±»åˆ«',
                        dataset_name='baseline_all_classes'
                    ),
                    ExpertConfig(
                        name='minority_expert',
                        target_classes=minority_classes,
                        description=f'å°‘æ•°ç±»ä¸“å®¶ï¼Œä¸“é—¨å¤„ç†ç±»åˆ« {minority_classes}',
                        dataset_name='minority_expert'
                    )
                ]
            }

        elif minority_count <= total_classes // 2:
            print(f"   ğŸ“Œ ç­–ç•¥: ä¸­ç­‰æ•°é‡å°‘æ•°ç±»({minority_count}ä¸ª)ï¼Œä½¿ç”¨åŸºå‡†æ¨¡å‹ + åˆ†ç»„å°‘æ•°ç±»ä¸“å®¶")

            # å¯¹å°‘æ•°ç±»è¿›è¡Œèšç±»åˆ†ç»„
            expert_groups = self._cluster_minority_classes(minority_classes, class_distribution)

            experts = [
                ExpertConfig(
                    name='baseline',
                    target_classes=list(class_distribution.keys()),
                    description='åŸºå‡†æ¨¡å‹ï¼Œå¤„ç†æ‰€æœ‰ç±»åˆ«',
                    dataset_name='baseline_all_classes'
                )
            ]

            for i, group in enumerate(expert_groups):
                experts.append(
                    ExpertConfig(
                        name=f'minority_expert_{i+1}',
                        target_classes=group,
                        description=f'å°‘æ•°ç±»ä¸“å®¶{i+1}ï¼Œå¤„ç†ç±»åˆ« {group}',
                        dataset_name=f'minority_expert_{i+1}'
                    )
                )

            return {
                'type': 'baseline_plus_grouped',
                'experts': experts
            }

        else:
            print(f"   ğŸ“Œ ç­–ç•¥: å¤§é‡å°‘æ•°ç±»({minority_count}ä¸ª)ï¼Œä½¿ç”¨å¤šå±‚ä¸“å®¶æ¶æ„")
            return self._design_hierarchical_experts(class_distribution, minority_classes)

    def _cluster_minority_classes(self, minority_classes: List[int],
                                class_distribution: Dict[int, int]) -> List[List[int]]:
        """
        å¯¹å°‘æ•°ç±»è¿›è¡Œèšç±»åˆ†ç»„

        Args:
            minority_classes: å°‘æ•°ç±»åˆ—è¡¨
            class_distribution: ç±»åˆ«åˆ†å¸ƒ

        Returns:
            åˆ†ç»„åçš„å°‘æ•°ç±»åˆ—è¡¨
        """
        minority_counts = {cls: class_distribution[cls] for cls in minority_classes}
        sorted_classes = sorted(minority_counts.items(), key=lambda x: x[1])

        # åŠ¨æ€ç¡®å®šèšç±»æ•°é‡
        if len(sorted_classes) <= 3:
            return [[cls for cls, _ in sorted_classes]]
        else:
            # æŒ‰æ•°é‡ç›¸ä¼¼æ€§åˆ†ç»„
            groups = []
            current_group = []
            current_count = None

            for cls, count in sorted_classes:
                if current_count is None or abs(count - current_count) / max(current_count, 1) < 0.5:
                    current_group.append(cls)
                    current_count = count
                else:
                    groups.append(current_group)
                    current_group = [cls]
                    current_count = count

            if current_group:
                groups.append(current_group)

            print(f"   ğŸ”€ å°‘æ•°ç±»åˆ†ç»„ç»“æœ: {groups}")
            return groups

    def _design_hierarchical_experts(self, class_distribution: Dict[int, int],
                                   minority_classes: List[int]) -> Dict:
        """
        è®¾è®¡å¤šå±‚ä¸“å®¶æ¶æ„

        Args:
            class_distribution: ç±»åˆ«åˆ†å¸ƒ
            minority_classes: å°‘æ•°ç±»åˆ—è¡¨

        Returns:
            å¤šå±‚ä¸“å®¶ç­–ç•¥é…ç½®
        """
        # æŒ‰æ ·æœ¬æ•°é‡å°†å°‘æ•°ç±»åˆ†ä¸º3ç»„
        minority_counts = {cls: class_distribution[cls] for cls in minority_classes}
        sorted_classes = sorted(minority_counts.items(), key=lambda x: x[1])

        n = len(sorted_classes)
        if n <= 3:
            groups = [[cls for cls, _ in sorted_classes]]
        else:
            # ä¸‰ç­‰åˆ†
            third = n // 3
            groups = [
                [cls for cls, _ in sorted_classes[:third]],
                [cls for cls, _ in sorted_classes[third:2*third]],
                [cls for cls, _ in sorted_classes[2*third:]]
            ]

        experts = [
            ExpertConfig(
                name='baseline',
                target_classes=list(class_distribution.keys()),
                description='åŸºå‡†æ¨¡å‹ï¼Œå¤„ç†æ‰€æœ‰ç±»åˆ«',
                dataset_name='baseline_all_classes'
            )
        ]

        for i, group in enumerate(groups):
            if group:  # ç¡®ä¿ç»„ä¸ä¸ºç©º
                experts.append(
                    ExpertConfig(
                        name=f'specialist_expert_{i+1}',
                        target_classes=group,
                        description=f'ä¸“é¡¹ä¸“å®¶{i+1}ï¼Œå¤„ç†ç±»åˆ« {group}',
                        dataset_name=f'specialist_expert_{i+1}'
                    )
                )

        return {
            'type': 'hierarchical',
            'experts': experts
        }

    def create_expert_datasets(self, expert_strategy: Dict) -> Dict[str, str]:
        """
        ä¸ºæ¯ä¸ªä¸“å®¶åˆ›å»ºä¸“ç”¨æ•°æ®é›†

        Args:
            expert_strategy: ä¸“å®¶ç­–ç•¥é…ç½®

        Returns:
            ä¸“å®¶æ•°æ®é›†è·¯å¾„å­—å…¸ {expert_name: dataset_path}
        """
        print("\nğŸ“¦ åˆ›å»ºä¸“å®¶ä¸“ç”¨æ•°æ®é›†...")

        dataset_paths = {}

        for expert_config in expert_strategy['experts']:
            print(f"   ğŸ”„ åˆ›å»ºæ•°æ®é›†: {expert_config.name}")

            # æ„å»ºæ•°æ®é›†åˆ›å»ºå‘½ä»¤
            output_dir = f"train_test_data/{expert_config.dataset_name}"

            cmd = [
                "python", "create_train_test_set.py",
                "--source_dir", self.data_source_dir,
                "--output_dir", output_dir,
                "--experiment_type", "imbalanced"  # ä¿æŒåŸå§‹åˆ†å¸ƒ
            ]

            # å¦‚æœæ˜¯å°‘æ•°ç±»ä¸“å®¶ï¼ŒæŒ‡å®šç›®æ ‡ç±»åˆ«
            if "minority" in expert_config.name or "specialist" in expert_config.name:
                classes_str = ",".join(map(str, expert_config.target_classes))
                cmd.extend(["--minority-classes", classes_str])

            try:
                print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                print(f"   âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {output_dir}")
                dataset_paths[expert_config.name] = output_dir

            except subprocess.CalledProcessError as e:
                print(f"   âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
                print(f"   é”™è¯¯è¾“å‡º: {e.stderr}")
                continue

        return dataset_paths

    def train_expert_models(self, expert_strategy: Dict,
                          dataset_paths: Dict[str, str]) -> Dict[str, str]:
        """
        è®­ç»ƒä¸“å®¶æ¨¡å‹

        Args:
            expert_strategy: ä¸“å®¶ç­–ç•¥é…ç½®
            dataset_paths: æ•°æ®é›†è·¯å¾„å­—å…¸

        Returns:
            ä¸“å®¶æ¨¡å‹è·¯å¾„å­—å…¸ {expert_name: model_path}
        """
        print("\nğŸ¤– è®­ç»ƒä¸“å®¶æ¨¡å‹...")

        model_paths = {}

        for expert_config in expert_strategy['experts']:
            if expert_config.name not in dataset_paths:
                print(f"   âš ï¸  è·³è¿‡ {expert_config.name}: ç¼ºå°‘æ•°æ®é›†")
                continue

            print(f"   ğŸ”„ è®­ç»ƒæ¨¡å‹: {expert_config.name}")

            dataset_path = dataset_paths[expert_config.name]
            model_dir = f"trained_models/{expert_config.name}"

            # æ„å»ºè®­ç»ƒå‘½ä»¤
            cmd = [
                "python", "train_resnet.py",
                "--train_data", f"{dataset_path}/train.parquet",
                "--test_data", f"{dataset_path}/test.parquet",
                "--output_dir", model_dir,
                "--max_epochs", "50",
                "--task", "traffic"
            ]

            try:
                print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)

                # æŸ¥æ‰¾ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
                if os.path.exists(model_dir):
                    ckpt_files = [f for f in os.listdir(model_dir) if f.endswith('.ckpt')]
                    if ckpt_files:
                        model_path = os.path.join(model_dir, ckpt_files[0])
                        model_paths[expert_config.name] = model_path
                        print(f"   âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ: {model_path}")
                    else:
                        print(f"   âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶åœ¨ {model_dir}")
                else:
                    print(f"   âš ï¸  æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")

            except subprocess.CalledProcessError as e:
                print(f"   âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                print(f"   é”™è¯¯è¾“å‡º: {e.stderr}")
                continue

        return model_paths

    def optimize_ensemble_weights(self, expert_strategy: Dict,
                                model_paths: Dict[str, str]) -> Dict:
        """
        ä¼˜åŒ–é›†æˆæƒé‡

        Args:
            expert_strategy: ä¸“å®¶ç­–ç•¥é…ç½®
            model_paths: æ¨¡å‹è·¯å¾„å­—å…¸

        Returns:
            ä¼˜åŒ–åçš„é…ç½®
        """
        print("\nâš–ï¸  ä¼˜åŒ–é›†æˆæƒé‡...")

        # ç®€å•çš„æƒé‡åˆ†é…ç­–ç•¥
        # åŸºå‡†æ¨¡å‹è·å¾—åŸºç¡€æƒé‡ï¼Œä¸“å®¶æ¨¡å‹æ ¹æ®å…¶ä¸“ä¸šæ€§è·å¾—é¢å¤–æƒé‡

        total_experts = len(model_paths)
        if total_experts == 1:
            weights = {name: 1.0 for name in model_paths.keys()}
        elif total_experts == 2:
            # åŸºå‡†æ¨¡å‹ + å•ä¸€ä¸“å®¶
            baseline_weight = 0.8
            expert_weight = 0.2
            weights = {}
            for name in model_paths.keys():
                if 'baseline' in name:
                    weights[name] = baseline_weight
                else:
                    weights[name] = expert_weight
        else:
            # å¤šä¸ªä¸“å®¶çš„æƒé‡åˆ†é…
            baseline_weight = 0.6
            remaining_weight = 0.4
            expert_weight = remaining_weight / (total_experts - 1)

            weights = {}
            for name in model_paths.keys():
                if 'baseline' in name:
                    weights[name] = baseline_weight
                else:
                    weights[name] = expert_weight

        print("   æƒé‡åˆ†é…:")
        for name, weight in weights.items():
            print(f"     {name}: {weight:.3f}")

        return {
            'strategy': expert_strategy,
            'model_paths': model_paths,
            'weights': weights
        }

    def run_full_pipeline(self) -> AnalysisResults:
        """
        è¿è¡Œå®Œæ•´çš„è‡ªåŠ¨åŒ–ä¸“å®¶ç³»ç»Ÿæµç¨‹

        Returns:
            åˆ†æç»“æœ
        """
        print("=" * 80)
        print("ğŸš€ å¯åŠ¨è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿ")
        print("=" * 80)

        try:
            # Phase 1: æ•°æ®åˆ†å¸ƒåˆ†æ
            print("\n" + "="*50)
            print("ğŸ“Š Phase 1: æ•°æ®åˆ†å¸ƒåˆ†æ")
            print("="*50)

            class_distribution = self.analyze_class_distribution()
            stats = self.calculate_distribution_statistics(class_distribution)

            # Phase 2: å°‘æ•°ç±»è¯†åˆ«
            print("\n" + "="*50)
            print("ğŸ¯ Phase 2: å°‘æ•°ç±»è‡ªåŠ¨è¯†åˆ«")
            print("="*50)

            minority_classes = self.identify_minority_classes(class_distribution, stats)

            # Phase 3: ä¸“å®¶ç­–ç•¥è®¾è®¡
            print("\n" + "="*50)
            print("ğŸ§  Phase 3: ä¸“å®¶ç­–ç•¥è®¾è®¡")
            print("="*50)

            expert_strategy = self.design_expert_strategy(class_distribution, minority_classes)

            # Phase 4: æ•°æ®é›†åˆ›å»º
            print("\n" + "="*50)
            print("ğŸ“¦ Phase 4: ä¸“å®¶æ•°æ®é›†åˆ›å»º")
            print("="*50)

            dataset_paths = self.create_expert_datasets(expert_strategy)

            # Phase 5: æ¨¡å‹è®­ç»ƒ
            print("\n" + "="*50)
            print("ğŸ¤– Phase 5: ä¸“å®¶æ¨¡å‹è®­ç»ƒ")
            print("="*50)

            model_paths = self.train_expert_models(expert_strategy, dataset_paths)

            # Phase 6: æƒé‡ä¼˜åŒ–
            print("\n" + "="*50)
            print("âš–ï¸  Phase 6: é›†æˆæƒé‡ä¼˜åŒ–")
            print("="*50)

            final_config = self.optimize_ensemble_weights(expert_strategy, model_paths)

            # ä¿å­˜ç»“æœ
            results = AnalysisResults(
                class_distribution=class_distribution,
                total_samples=sum(class_distribution.values()),
                total_classes=len(class_distribution),
                statistics=stats,
                minority_classes=minority_classes,
                expert_strategy=final_config
            )

            # ä¿å­˜é…ç½®æ–‡ä»¶
            self.save_results(results)

            print("\n" + "="*80)
            print("ğŸ‰ è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿæ‰§è¡Œå®Œæˆ!")
            print("="*80)

            return results

        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}")
            raise
        finally:
            if self.spark:
                self.spark.stop()

    def save_results(self, results: AnalysisResults):
        """
        ä¿å­˜åˆ†æç»“æœ

        Args:
            results: åˆ†æç»“æœ
        """
        print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

        # ä¿å­˜è¯¦ç»†é…ç½®
        config_file = os.path.join(self.output_base_dir, "auto_expert_config.json")

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_config = {
            'strategy': results.expert_strategy['strategy'].type if hasattr(results.expert_strategy['strategy'], 'type') else 'unknown',
            'experts': [
                {
                    'name': expert.name,
                    'target_classes': expert.target_classes,
                    'description': expert.description,
                    'dataset_name': expert.dataset_name,
                    'model_path': expert.model_path
                }
                for expert in results.expert_strategy['strategy'].experts if hasattr(results.expert_strategy['strategy'], 'experts')
            ],
            'model_paths': results.expert_strategy['model_paths'],
            'weights': results.expert_strategy['weights']
        }

        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_config, f, indent=2, ensure_ascii=False)

        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_file = os.path.join(self.output_base_dir, "analysis_report.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿåˆ†ææŠ¥å‘Š\n")
            f.write("="*50 + "\n\n")

            f.write(f"æ•°æ®æºç›®å½•: {self.data_source_dir}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {results.total_samples:,}\n")
            f.write(f"æ€»ç±»åˆ«æ•°: {results.total_classes}\n")
            f.write(f"å°‘æ•°ç±»: {results.minority_classes}\n\n")

            f.write("ç±»åˆ«åˆ†å¸ƒè¯¦æƒ…:\n")
            for class_id, count in sorted(results.class_distribution.items()):
                class_name = ID_TO_TRAFFIC.get(class_id, f"Unknown-{class_id}")
                marker = " [å°‘æ•°ç±»]" if class_id in results.minority_classes else ""
                f.write(f"  ç±»åˆ« {class_id} ({class_name}){marker}: {count:,} æ ·æœ¬\n")

            f.write(f"\nåˆ†å¸ƒç»Ÿè®¡ç‰¹å¾:\n")
            for key, value in results.statistics.items():
                f.write(f"  {key}: {value:,.3f}\n")

            f.write(f"\nä¸“å®¶ç­–ç•¥é…ç½®:\n")
            f.write(f"  æ¨¡å‹è·¯å¾„: {results.expert_strategy['model_paths']}\n")
            f.write(f"  æƒé‡åˆ†é…: {results.expert_strategy['weights']}\n")

        print(f"   âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_file}")
        print(f"   âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è‡ªåŠ¨åŒ–ä¸“å®¶é€‰æ‹©ç³»ç»Ÿ")
    parser.add_argument("--data_source", type=str,
                       default="processed_data/vpn",
                       help="é¢„å¤„ç†æ•°æ®æºç›®å½•")
    parser.add_argument("--output_dir", type=str,
                       default="auto_expert_results",
                       help="è¾“å‡ºåŸºç¡€ç›®å½•")
    parser.add_argument("--config", type=str,
                       help="ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)")

    args = parser.parse_args()

    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    auto_system = AutoExpertSystem(args.data_source, args.output_dir)

    # åŠ è½½è‡ªå®šä¹‰é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            custom_config = json.load(f)
            auto_system.config.update(custom_config)
        print(f"âœ… å·²åŠ è½½è‡ªå®šä¹‰é…ç½®: {args.config}")

    # è¿è¡Œå®Œæ•´æµç¨‹
    results = auto_system.run_full_pipeline()

    print("\nğŸ¯ æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨¡å‹è¯„ä¼°:")
    print("python evaluation.py --eval-mode ensemble --data_path <test_data> \\")
    print("                    --baseline_model_path <baseline_model> \\")
    print("                    --minority_model_path <minority_model> \\")
    print("                    --minority_classes <minority_classes>")

if __name__ == "__main__":
    main()