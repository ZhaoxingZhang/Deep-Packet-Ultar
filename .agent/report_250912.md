
# 研究进展报告

## 0. 前置调研

### 关于基准模型的调研

| 📁 Ae 模型名称 | 🔗 GitHub | 📖 论文会议 | # USTC-TFC2016 | # ISCXVPN2016 | # ISCXTor2016 | # CICIoT2022 | # CrossPlatform(iOS) | CrossPlatform(Android) | 📝 备注 |
|---|---|---|---|---|---|---|---|---|---|
| 1 NetMamba | [wangtz19/NetMamba](https://github.com/wangtz19/NetMamba) | ICNP 2024 (A类) | 99.57% | 98.06% | 99.86% | 99.29% | 93.05% | 90.96% |  |
| 2 ET-BERT | [linwhitehat/ET-BERT](https://github.com/linwhitehat/ET-BERT) | WWW 2022 (A类) |  | 98.90% | 99.20% |  |  | 92.50% |  |
| 3 YaTC | [https://github.com/NSSL-SJTU/YaTC](https://github.com/NSSL-SJTU/YaTC) | AAAI 2023 (A类) | 97.86% | 98.04% | 99.72% | 96.58% | 82.35% |  | YaTC是一种基于掩码自编码器和多层次流表示的... |
| 4 Deep Packet | [https://github.com/munhoujian/Deep-Packet](https://github.com/munhoujian/Deep-Packet) | Soft Computing 2022（会议） |  | 98.00% |  |  |  |  |  |
| 5 AutoML4ETC | [https://github.com/OrangeUW/AutoML4ETC](https://github.com/OrangeUW/AutoML4ETC) | TNSM 2023 (B类) |  | 94.40% |  |  |  |  |  |



前期选择NetMamba作为基础模型，原因是准确率最高且发布年限最近，但是实际修改中发现模型存在CUDA kernel 硬绑定、自定义算子、不支持在线检测等问题，修改难度较高，因此改为选取表现较好、架构也最简单的 Deep Packet模型



## 1. 总体研究路径概览

| 阶段 | 实验/任务 | 核心目的 | 关键发现/产出 |
| :--- | :--- | :--- | :--- |
| **第一阶段** | `exp1`, `exp2`, `exp3` | 在小型、快速迭代的数据集上，分别对“开放集识别”、“数据不均衡”等核心研究问题进行可行性验证。 | **洞察**：证明了小型、人工构造的数据集对于模型评估存在严重误导性，其上的实验结论不可靠。 |
| **第二阶段** | `exp4`, `exp5` | 演进至更大、更真实的（不均衡）数据集，并建立可靠的实验基准。 | **洞察**：发现标准ResNet性能远超添加了注意力机制的复杂模型，确立了新的基准模型和性能瓶颈（少数类识别）。 |
| **&nbsp;** | *工程重构* | 解决大数据集带来的内存溢出和训练耗时问题。 | **产出**：实现了支持分批处理的数据脚本和带有周期验证的训练流程，保障了后续研究的可行性。 |
| **当前阶段** | `exp6` | 采用数据中心方法，直接应对“数据不均衡”这一核心挑战。 | **产出**：已成功利用SMOTE算法生成了类别均衡的`exp6_smote`训练集，即将开始模型训练与评估。 |

---

## 2. 研究背景与数据集

*   **数据集**: 所有实验均基于 **ISCXVPN2016 NonVPN 数据集**。
*   **核心研究问题**:
    1.  **开放集识别 (Open-Set Recognition)**: 模型在面对训练期间从未见过的、全新的流量类别时，应如何有效地将其识别为“未知”，而非错误地归类到任何一个已知类别中。
    2.  **数据不均衡适应性 (Imbalanced Data Adaptability)**: 在类别样本数量分布悬殊的真实网络环境下，模型应如何保持其分类性能的稳健性。

---

## 3. 详细实验迭代过程

### 3.1. 第一阶段：基于小型数据集的可行性与基线探索

此阶段的目标是在一系列小规模、可快速处理的数据集上，分别对上述两个核心研究问题进行初步探索，并建立一个“控制变量”基准。

#### 3.1.1. 实验设计

此阶段设计的三个关联子实验及其核心思路如下表所示：

| 实验代号 | 关联研究问题 | 核心目的 | 数据集构建方法 |
| :--- | :--- | :--- | :--- |
| `exp1_small_test` | 开放集识别 | 验证开放集识别方案的技术可行性。 | 每类流量选取一个文件，构建小型均衡数据集。 |
| `exp2_imbalanced_small` | 数据不均衡适应性 | 观察不均衡数据对模型评估指标的影响。 | 人工构造一个类别数量差异巨大的小型数据集。 |
| `exp3_balanced_small` | 基准性能/控制变量 | 检验模型在无干扰的“标准”均衡场景下的真实性能。 | 构建一个与`exp2`规模相仿但类别完全均衡的数据集。 |

#### 3.1.2. 实验结果与分析

| 实验代号 | 关联研究问题 | 关键结果与结论 |
| :--- | :--- | :--- |
| `exp1_small_test` | 开放集识别 | 验证了数据处理、模型训练、评估等整个技术管线的通畅性。但因数据集过小，评估结果无统计学意义。 |
| `exp2_imbalanced_small` | 数据不均衡适应性 | 获得了具有高度误导性的**75.5%**准确率。深入分析发现模型严重偏向多数类，对少数类缺乏有效的识别能力。 |
| `exp3_balanced_small` | 控制实验/基准性能 | 准确率仅为**59%**。该结果揭示了模型在理想均衡环境下的真实基础性能，证明了`exp2`的高准确率是虚假的。 |

#### 3.1.3. 阶段性洞察

| 洞察类别 | 具体内容 |
| :--- | :--- |
| **关于实验基准** | **小型、人工构造的数据集是不可靠的**。其规模不足以为模型（特别是较复杂的模型）提供足够的学习信号，其上的实验结论不具备推广性，甚至会产生误导（如在`exp3`上错误地否定了注意力机制）。 |
| **关于评估指标** | 在不均衡数据集上，总体准确率（Accuracy）是一个高度不可靠的“虚荣指标”。**宏平均F1分数（Macro F1-Score）**等能够平等对待每个类别的指标，是评估模型真实性能的关键。 |

---

### 3.2. 第二阶段：向可靠基准的演进

基于第一阶段的洞察，此阶段的核心目标是建立一个规模和分布都更接近真实场景的、可靠的实验基准，并在此之上重新进行模型选型。

#### 3.2.1. 面临的挑战与工程实践

在转向更大规模数据集（全量数据的1/600，约2.4万训练样本）的过程中，遇到了两个关键的工程挑战：

1.  **数据生成瓶颈**: 直接使用Spark对大规模数据进行全局操作，会导致`SparkOutOfMemoryError`。
    *   **解决方案**: 对`create_train_test_set.py`进行重构，实现了**分批处理与精确比例采样**功能。新流程通过循环处理小批量文件并合并采样结果，在避免内存溢出的同时，实现了对最终数据集大小的精确控制。
2.  **训练效率瓶颈**: 新数据集使单次训练（epoch）耗时超过1.5小时，严重影响研究效率。
    *   **解决方案**: 对训练框架（PyTorch Lightning）的调用方式进行重构，为训练流程引入了**周期性验证（per-epoch validation）机制**。通过在每个epoch后评估模型在独立验证集上的性能，并利用TensorBoard进行可视化，为科学地判断模型收敛点、缩减不必要的训练时长提供了数据支持。

#### 3.2.2. 模型性能对比 (`exp5`)

在新的`exp5_fractional_1_600`不均衡数据集上，对标准ResNet模型及添加了SEBlock注意力机制的ResNet模型进行了性能对比。

| 模型 | 准确率 | 宏平均F1 | 宏平均Precision | 宏平均Recall |
| :--- | :--- | :--- | :--- | :--- |
| **标准 ResNet** | **<font color='green'>90.8%</font>** | **<font color='green'>0.72</font>** | **<font color='green'>0.73</font>** | **<font color='green'>0.72</font>** |
| ResNet + Attention | 86.2% | 0.52 | 0.52 | 0.55 |

#### 3.2.3. 阶段性洞察

| 洞察类别 | 具体内容 |
| :--- | :--- |
| **关于模型选型** | 标准的ResNet架构在当前任务中表现出强大的鲁棒性和学习能力，是一个非常高的性能基准。 |
| **关于注意力机制** | SEBlock注意力机制在当前不均衡的数据分布下，对模型性能有显著的负面影响。推断其可能被多数类信号主导，从而抑制了对少数类特征的有效学习。 |
| **关于性能瓶颈** | 模型架构已不再是性能的主要瓶颈。当前的核心挑战已明确为**数据不均衡**导致的极端少数类（如类别`0`, `11`）识别失败问题。 |

---

## 4. 当前阶段与下一步计划

### 4.1. 当前最佳模型与核心挑战

*   **当前最佳模型**: 标准ResNet（无注意力机制）。
*   **当前最佳性能**: 在`exp5`不均衡测试集上，总体准确率90.8%，宏平均F1分数**0.72**。
*   **核心挑战**: 提升对极端少数类的识别能力，将宏平均F1分数从0.72的水平进一步提高。

### 4.2. 下一步计划：基于SMOTE的数据中心方法

*   **方向**: 既然核心挑战是数据问题，下一步计划将采用数据中心（Data-Centric）的方法，通过优化训练集来提升模型性能。
*   **具体任务**: 采用SMOTE（合成少数类过采样技术）对`exp5`的训练集进行处理，人工生成少数类的样本，从而创建一个类别完全均衡的训练集。
*   **当前状态**: 
    *   `apply_smote.py`脚本已开发并调试完毕。
    *   已成功生成平衡后的训练数据集`exp6_smote`。
*   **待执行动作**: 
    1.  使用当前最佳模型（标准ResNet），在`exp6_smote`数据集上进行训练。
    2.  在原始的、不均衡的`exp5`测试集上进行评估，检验SMOTE方法对少数类识别能力的提升效果。

---

## 5. 经验与教训总结

| 方面 | 经验与教训 |
| :--- | :--- |
| **实验基准** | **小型或人工构造的数据集是不可靠的**。它们可能无法提供足够的信号来评估复杂模型，甚至会得出与在真实数据分布下截然相反的结论。建立一个规模充足、分布真实的基准数据集是进行科学模型研究的必要前提。 |
| **评估指标** | **宏平均F1分数远比总体准确率更重要**。在类别不均衡的任务中，必须使用能够平等对待所有类别的指标，才能真实地衡量模型的泛化能力，避免被多数类带来的虚高准确率所误导。 |
| **模型复杂度** | **奥卡姆剃刀原理同样适用于此**：更复杂的模型不总是更好。在当前任务中，一个更简洁的ResNet架构表现比增加了注意力机制的复杂版本更为鲁棒。必须警惕复杂组件在特定数据分布下可能带来的负面效应。 |
| **工程实践** | **可扩展的数据管线和可观测的训练流程是研究的加速器**。通过对数据生成脚本和训练脚本的重构，解决了内存瓶颈和效率瓶颈，使得在更大规模数据集上进行快速、可靠的实验成为可能。 |
