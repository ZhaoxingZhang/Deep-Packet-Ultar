#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼šè®­ç»ƒå’Œè¯„ä¼°ä¸åŒå‚æ•°çš„é—¨æ§ç½‘ç»œ

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. è®­ç»ƒå¤šä¸ªä¸åŒÎ»å€¼çš„é—¨æ§ç½‘ç»œ
2. è®­ç»ƒè‡ªé€‚åº”è°ƒåº¦çš„é—¨æ§ç½‘ç»œ
3. ä½¿ç”¨è¯„ä¼°è„šæœ¬æµ‹è¯•æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½
4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import warnings
warnings.filterwarnings("ignore", "pkg_resources is deprecated")

import os
import subprocess
import json
import time
import click
from datetime import datetime
from ruamel.yaml import YAML

# åˆ›å»ºlogç›®å½•
LOG_DIR = "log"
os.makedirs(LOG_DIR, exist_ok=True)

# é…ç½®å‚æ•°
PYTHON_PATH = "python"

# é»˜è®¤åŸºç¡€é…ç½®
DEFAULT_BASE_CONFIG = {
    "train_data_path": "train_test_data/exp2_imbalanced_small/traffic_classification/train.parquet",
    "baseline_model_path": "model/resnet_traffic_featurize.model.ckpt",
    "minority_model_path": "model/minority_expert_resnet.pth.ckpt",
    "minority_classes": [5, 7],
    "epochs": 10,
    "lr": 0.001
}

# é»˜è®¤æµ‹è¯•é…ç½®
DEFAULT_TEST_CONFIGS = [
    {
        "name": "lambda_0.1",
        "output_path": "model/model/gating_network_lambda_0.1.ckpt",
        "lambda_macro": 0.1
    },
    {
        "name": "lambda_0.3",
        "output_path": "model/model/gating_network_lambda_0.3.ckpt",
        "lambda_macro": 0.3
    },
    {
        "name": "lambda_0.5",
        "output_path": "model/model/gating_network_lambda_0.5.ckpt",
        "lambda_macro": 0.5
    },
    {
        "name": "lambda_0.7",
        "output_path": "model/model/gating_network_lambda_0.7.ckpt",
        "lambda_macro": 0.7
    },
    {
        "name": "adaptive",
        "output_path": "model/model/gating_network_adaptive.ckpt",
        "use_adaptive": True,
        "initial_lambda": 0.1,
        "final_lambda": 0.7
    }
]

def load_config(config_file_path):
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    yaml = YAML()
    try:
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = yaml.load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file_path}")
        return None
    except Exception as e:
        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None

def merge_configs(default_config, user_config):
    """åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®"""
    merged = default_config.copy()
    if user_config:
        merged.update(user_config)
    return merged


def run_training_command(config):
    """æ‰§è¡Œå•ä¸ªè®­ç»ƒå‘½ä»¤"""
    cmd = [
        PYTHON_PATH,
        "train_gating_network.py",
        "--train_data_path", config["train_data_path"],
        "--baseline_model_path", config["baseline_model_path"],
        "--minority_model_path", config["minority_model_path"],
        "--output_path", config["output_path"],
        "--epochs", str(config["epochs"]),
        "--lr", str(config["lr"])
    ]

    # æ·»åŠ minority classes
    for minority_class in config["minority_classes"]:
        cmd.extend(["--minority_classes", str(minority_class)])

    # æ·»åŠ Î»ç›¸å…³å‚æ•°
    if config.get("use_adaptive", False):
        cmd.extend([
            "--use_adaptive",
            "--initial_lambda", str(config["initial_lambda"]),
            "--final_lambda", str(config["final_lambda"])
        ])
    else:
        cmd.extend(["--lambda_macro", str(config["lambda_macro"])])

    print(f"\n=== å¼€å§‹è®­ç»ƒ: {config['name']} ===")
    print(f"å‘½ä»¤: {' '.join(cmd)}")

    start_time = time.time()

    # è¿è¡Œå‘½ä»¤å¹¶å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                             text=True, universal_newlines=True)

    output_lines = []
    for line in iter(process.stdout.readline, ''):
        output_lines.append(line.strip())
        print(line.strip(), end='')  # å®æ—¶æ˜¾ç¤º

    process.wait()
    result = subprocess.CompletedProcess(process.args, process.returncode,
                                       ''.join(output_lines), '')

    end_time = time.time()
    training_time = end_time - start_time

    if result.returncode == 0:
        print(f"\nâœ… {config['name']} è®­ç»ƒæˆåŠŸ (è€—æ—¶: {training_time:.1f}ç§’)")
        return {
            "success": True,
            "training_time": training_time,
            "output": result.stdout
        }
    else:
        print(f"\nâŒ {config['name']} è®­ç»ƒå¤±è´¥ (è€—æ—¶: {training_time:.1f}ç§’)")
        return {
            "success": False,
            "training_time": training_time,
            "output": result.stdout
        }

def run_evaluation(model_path, model_name, test_data_path, baseline_model_path, minority_model_path, minority_classes):
    """è¿è¡Œè¯„ä¼°è„šæœ¬"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(LOG_DIR, f"evaluation_results_{model_name}")
    os.makedirs(output_dir, exist_ok=True)

    cmd = [PYTHON_PATH, "evaluation.py"]

    # æ·»åŠ æ‰€æœ‰å‚æ•°
    cmd.extend(["--data_path", test_data_path])
    cmd.extend(["--output_dir", output_dir])
    cmd.extend(["--baseline_model_path", baseline_model_path])
    cmd.extend(["--minority_model_path", minority_model_path])
    cmd.extend(["--gating_network_path", model_path])
    cmd.extend(["--eval-mode", "gating_ensemble"])

    # ä¸ºæ¯ä¸ªminority_classæ·»åŠ å•ç‹¬çš„--minority_classeså‚æ•°
    for c in minority_classes:
        cmd.extend(["--minority_classes", str(c)])

    print(f"\n=== è¯„ä¼°æ¨¡å‹: {model_name} ===")
    print(f"å‘½ä»¤: {' '.join(cmd)}")

    start_time = time.time()

    # è¿è¡Œå‘½ä»¤å¹¶å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                             text=True, universal_newlines=True)

    output_lines = []
    for line in iter(process.stdout.readline, ''):
        output_lines.append(line.strip())
        print(line.strip(), end='')  # å®æ—¶æ˜¾ç¤º

    process.wait()
    result = subprocess.CompletedProcess(process.args, process.returncode,
                                       ''.join(output_lines), '')

    end_time = time.time()
    evaluation_time = end_time - start_time

    if result.returncode == 0:
        print(f"\nâœ… {model_name} è¯„ä¼°æˆåŠŸ (è€—æ—¶: {evaluation_time:.1f}ç§’)")

        # è§£æè¯„ä¼°ç»“æœ
        try:
            output_text = result.stdout
            accuracy = None
            macro_avg = None
            minority_classes = {}

            for line in output_text.split('\n'):
                line = line.strip()

                # è§£æå‡†ç¡®ç‡
                if "Accuracy:" in line:
                    try:
                        accuracy = float(line.split(":")[1].strip())
                    except (ValueError, IndexError):
                        pass

                # è§£æmacro avg - f1-scoreåœ¨ç¬¬4ä¸ªä½ç½®(ç´¢å¼•3)
                elif "macro avg" in line:
                    try:
                        parts = line.split()
                        if len(parts) >= 4:
                            macro_avg = float(parts[3])  # f1-scoreåœ¨ç¬¬4ä¸ªä½ç½®
                    except (ValueError, IndexError):
                        pass

                # è§£æå°‘æ•°ç±»æŒ‡æ ‡ - æ”¯æŒæ‰€æœ‰å°‘æ•°ç±»
                elif line.isdigit():
                    class_id = int(line)
                    # æ‰¾åˆ°å°‘æ•°ç±»ï¼Œä¸‹ä¸€è¡Œæ˜¯æŒ‡æ ‡
                    try:
                        idx = output_text.split('\n').index(line)
                        if idx + 1 < len(output_text.split('\n')):
                            next_line = output_text.split('\n')[idx + 1].strip()
                            if "precision" in next_line and len(next_line.split()) >= 4:
                                metrics = next_line.split()
                                try:
                                    minority_classes[class_id] = {
                                        "precision": float(metrics[0]),
                                        "recall": float(metrics[1]),
                                        "f1": float(metrics[2])
                                    }
                                except ValueError:
                                    pass
                    except (ValueError, IndexError):
                        pass

            return {
                "success": True,
                "evaluation_time": evaluation_time,
                "accuracy": accuracy,
                "macro_avg": macro_avg,
                "minority_classes": minority_classes,
                "output": result.stdout
            }
        except Exception as e:
            print(f"âš ï¸ è§£æè¯„ä¼°ç»“æœæ—¶å‡ºé”™: {e}")
            return {
                "success": True,
                "evaluation_time": evaluation_time,
                "output": result.stdout
            }
    else:
        print(f"\nâŒ {model_name} è¯„ä¼°å¤±è´¥ (è€—æ—¶: {evaluation_time:.1f}ç§’)")
        return {
            "success": False,
            "evaluation_time": evaluation_time,
            "output": result.stdout,
            "error": result.stderr
        }

def generate_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ“Š é—¨æ§ç½‘ç»œæµ‹è¯•æŠ¥å‘Š")
    print("="*80)

    successful_trainings = [r for r in results if r["training"]["success"]]
    successful_evaluations = [r for r in results if r.get("evaluation", {}).get("success", False)]

    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæˆåŠŸ: {len(successful_trainings)}/{len(results)}")
    print(f"  è¯„ä¼°æˆåŠŸ: {len(successful_evaluations)}/{len(results)}")

    if successful_evaluations:
        print(f"\nğŸ† æ€§èƒ½æ’å (æŒ‰Macro-F1 Score):")

        # æŒ‰macro_avgæ’åºï¼Œè¿‡æ»¤æ‰Noneå€¼
        ranked = sorted(successful_evaluations,
                       key=lambda x: x["evaluation"].get("macro_avg") or 0,
                       reverse=True)

        for i, result in enumerate(ranked, 1):
            config = result["config"]
            eval_result = result["evaluation"]

            print(f"\n{i}. {config['name']}")
            print(f"   è®­ç»ƒæ—¶é—´: {result['training']['training_time']:.1f}ç§’")
            print(f"   è¯„ä¼°æ—¶é—´: {eval_result['evaluation_time']:.1f}ç§’")
            print(f"   å‡†ç¡®ç‡: {eval_result.get('accuracy', 'N/A'):.4f}")
            print(f"   Macro-F1 Score: {eval_result.get('macro_avg', 'N/A'):.4f}")

            # æ˜¾ç¤ºå°‘æ•°ç±»è¡¨ç°
            minority_classes = eval_result.get("minority_classes", {})
            if minority_classes:
                print(f"   å°‘æ•°ç±»è¡¨ç°:")
                for class_id, metrics in minority_classes.items():
                    print(f"     ç±»åˆ«{class_id}: F1={metrics.get('f1', 0):.4f}, "
                          f"Recall={metrics.get('recall', 0):.4f}")

    print(f"\nğŸ’¡ å»ºè®®:")
    if successful_evaluations:
        best_model = max(successful_evaluations,
                        key=lambda x: x["evaluation"].get("macro_avg", 0))
        print(f"  æœ€ä½³æ¨¡å‹: {best_model['config']['name']}")
        print(f"  æ¨èä½¿ç”¨: {best_model['config']['output_path']}")

        # åˆ†æÎ»å€¼è¶‹åŠ¿
        lambda_models = [r for r in successful_evaluations
                        if not r["config"].get("use_adaptive", False)]
        if len(lambda_models) >= 2:
            print(f"  Î»å€¼åˆ†æ: ä¸åŒÎ»å€¼å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“")
            print(f"  è‡ªé€‚åº”è°ƒåº¦: æ¯”å›ºå®šÎ»å€¼ {'æ›´å¥½' if any(r['config'].get('use_adaptive') for r in successful_evaluations) else 'éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜'}")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_models": len(results),
            "successful_trainings": len(successful_trainings),
            "successful_evaluations": len(successful_evaluations)
        },
        "results": []
    }

    for result in results:
        report_data["results"].append({
            "config": result["config"],
            "training": {
                "success": result["training"]["success"],
                "time": result["training"]["training_time"],
                "error": result["training"].get("error", "")
            },
            "evaluation": result.get("evaluation", {})
        })

    report_file = os.path.join(LOG_DIR, f"gating_network_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # åŒæ—¶ç”Ÿæˆå¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š
    text_report_file = os.path.join(LOG_DIR, f"gating_network_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    with open(text_report_file, 'w', encoding='utf-8') as f:
        f.write("é—¨æ§ç½‘ç»œæµ‹è¯•æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("æ€»ä½“ç»Ÿè®¡:\n")
        f.write(f"  è®­ç»ƒæˆåŠŸ: {len(successful_trainings)}/{len(results)}\n")
        f.write(f"  è¯„ä¼°æˆåŠŸ: {len(successful_evaluations)}/{len(results)}\n\n")

        if successful_evaluations:
            f.write("æ€§èƒ½æ’å (æŒ‰Macro-F1):\n")
            f.write("-"*60 + "\n")

            ranked = sorted(successful_evaluations,
                           key=lambda x: x["evaluation"].get("macro_avg", 0),
                           reverse=True)

            for i, result in enumerate(ranked, 1):
                config = result["config"]
                eval_result = result["evaluation"]

                f.write(f"\n{i}. {config['name']}\n")
                f.write(f"   è®­ç»ƒæ—¶é—´: {result['training']['training_time']:.1f}ç§’\n")
                f.write(f"   è¯„ä¼°æ—¶é—´: {eval_result['evaluation_time']:.1f}ç§’\n")
                f.write(f"   å‡†ç¡®ç‡: {eval_result.get('accuracy', 'N/A'):.4f}\n")
                f.write(f"   Macro-F1: {eval_result.get('macro_avg', 'N/A'):.4f}\n")

                # æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
                f.write(f"   è®­ç»ƒæ—¥å¿—: {result['training'].get('log_file', 'N/A')}\n")
                f.write(f"   è¯„ä¼°æ—¥å¿—: {eval_result.get('log_file', 'N/A')}\n")

                # æ˜¾ç¤ºå°‘æ•°ç±»è¡¨ç°
                minority_classes = eval_result.get("minority_classes", {})
                if minority_classes:
                    f.write(f"   å°‘æ•°ç±»è¡¨ç°:\n")
                    for class_id, metrics in minority_classes.items():
                        f.write(f"     ç±»åˆ«{class_id}: F1={metrics.get('f1', 0):.4f}, ")
                        f.write(f"Recall={metrics.get('recall', 0):.4f}\n")

        f.write(f"\nå»ºè®®:\n")
        if successful_evaluations:
            best_model = max(successful_evaluations,
                            key=lambda x: x["evaluation"].get("macro_avg", 0))
            f.write(f"  æœ€ä½³æ¨¡å‹: {best_model['config']['name']}\n")
            f.write(f"  æ¨èä½¿ç”¨: {best_model['config']['output_path']}\n")

    print(f"ğŸ“„ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {text_report_file}")

@click.command()
@click.option("--config", required=True, help="Path to the YAML configuration file.")
def run_test_gating_networks(config):
    """
    è‡ªåŠ¨åŒ–æµ‹è¯•é—¨æ§ç½‘ç»œä¸åŒå‚æ•°çš„æ€§èƒ½

    è¿™ä¸ªè„šæœ¬ä¼šï¼š
    1. ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
    2. è®­ç»ƒå¤šä¸ªä¸åŒÎ»å€¼çš„é—¨æ§ç½‘ç»œ
    3. è®­ç»ƒè‡ªé€‚åº”è°ƒåº¦çš„é—¨æ§ç½‘ç»œ
    4. ä½¿ç”¨è¯„ä¼°è„šæœ¬æµ‹è¯•æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½
    5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

    é…ç½®æ–‡ä»¶æ ¼å¼ç¤ºä¾‹:
    base_config:
      train_data_path: "train_test_data/exp2_imbalanced_small/traffic_classification/train.parquet"
      baseline_model_path: "model/resnet_traffic_featurize.model.ckpt"
      minority_model_path: "model/minority_expert_resnet.pth.ckpt"
      minority_classes: [5, 7]
      epochs: 10
      lr: 0.001

    test_configs:
      - name: "lambda_0.1"
        output_path: "model/model/gating_network_lambda_0.1.ckpt"
        lambda_macro: 0.1
      - name: "lambda_0.5"
        output_path: "model/model/gating_network_lambda_0.5.ckpt"
        lambda_macro: 0.5
      - name: "adaptive"
        output_path: "model/model/gating_network_adaptive.ckpt"
        use_adaptive: true
        initial_lambda: 0.1
        final_lambda: 0.7
    """

    print("ğŸš€ å¼€å§‹é—¨æ§ç½‘ç»œè‡ªåŠ¨åŒ–æµ‹è¯•")
    print(f"é…ç½®æ–‡ä»¶: {config}")

    # åŠ è½½é…ç½®æ–‡ä»¶
    user_config = load_config(config)
    if user_config is None:
        return

    # åˆå¹¶åŸºç¡€é…ç½®
    base_config = merge_configs(DEFAULT_BASE_CONFIG, user_config.get('base_config', {}))

    # åˆå¹¶æµ‹è¯•é…ç½®ï¼ˆå¦‚æœç”¨æˆ·æä¾›äº†test_configsï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·é…ç½®ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    user_test_configs = user_config.get('test_configs')
    test_configs = user_test_configs if user_test_configs is not None else DEFAULT_TEST_CONFIGS

    print(f"åŸºç¡€é…ç½®:")
    for key, value in base_config.items():
        print(f"  {key}: {value}")
    print(f"\næµ‹è¯•é…ç½®: {len(test_configs)} ä¸ªæ¨¡å‹")
    for i, test_config in enumerate(test_configs, 1):
        print(f"  {i}. {test_config['name']}")
    print(f"æ—¥å¿—ç›®å½•: {os.path.abspath(LOG_DIR)}")
    print(f"æ‰€æœ‰è®­ç»ƒå’Œè¯„ä¼°æ—¥å¿—å°†ä¿å­˜åœ¨è¯¥ç›®å½•ä¸­")

    results = []

    for i, test_config in enumerate(test_configs, 1):
        print(f"\n{'='*60}")
        print(f"è¿›åº¦: {i}/{len(test_configs)} - {test_config['name']}")
        print('='*60)

        # åˆå¹¶åŸºç¡€é…ç½®å’Œæµ‹è¯•é…ç½®
        full_config = base_config.copy()
        full_config.update(test_config)

        # è®­ç»ƒ
        training_result = run_training_command(full_config)

        result_entry = {
            "config": test_config,
            "training": training_result
        }

        # å¦‚æœè®­ç»ƒæˆåŠŸï¼Œè¿›è¡Œè¯„ä¼°
        if training_result["success"]:
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ¨¡å‹æ–‡ä»¶ä¿å­˜å®Œæˆ
            time.sleep(2)

            if os.path.exists(test_config["output_path"]):
                # æ¨å¯¼æµ‹è¯•æ•°æ®è·¯å¾„
                test_data_path = base_config["train_data_path"].replace("/train.parquet", "/test.parquet")

                evaluation_result = run_evaluation(
                    model_path=test_config["output_path"],
                    model_name=test_config["name"],
                    test_data_path=test_data_path,
                    baseline_model_path=base_config["baseline_model_path"],
                    minority_model_path=base_config["minority_model_path"],
                    minority_classes=base_config["minority_classes"]
                )
                result_entry["evaluation"] = evaluation_result
            else:
                print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {test_config['output_path']}")
                result_entry["evaluation"] = {"success": False, "error": "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨"}

        results.append(result_entry)

        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
        time.sleep(3)

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results)

    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    run_test_gating_networks()