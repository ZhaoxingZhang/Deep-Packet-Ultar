# 研究背景

*   **核心课题**: 面向开放集识别的增量式加密流量分类技术的研究与实现。
*   **关键挑战**: 
    1.  **开放集识别**: 模型需要有效识别并归类从未见过的新流量类别（即“未知类”）。
    2.  **数据不均衡**: 在真实网络环境中，各类应用的流量数据通常是不均衡的，模型需要在此情况下依然保持稳健的性能。
    3.  **增量识别**: 模型可以适应新增的流量类型，并在少量样本的情况下习得一定的分类准确性 

# 指令
* 同一天的进展写在相同的二级标题里，不同日期的进展另起二级标题
* 只进行append，不要删除已经写入的进展信息
* "下一步工作"只记录讨论中发散出的、明确的下一步，不要自己编写或者猜想
* 进展记录要求：
  - 简洁为主，避免冗余描述
  - 技术细节优先，配置路径、新增文件、修改文件要明确列出
  - 避免记录编码bug等不影响研究方向的问题
  - 初步实验结果不要记录

# 进展
## 日期：2025年10月24日
### 当前研究进展总结：
现在在基于[Deep-Packet](https://github.com/ZhaoxingZhang/Deep-Packet-Ultar)这个模型的基础上进行改进，以达到提升模型对未知流量的开放集识别能力，并改善其增量识别能力的目标，但是我这段时间做了多种尝试，主要有这些：

   1. 增加注意力机制：在ResNet模型中引入SEBlock注意力机制后，模型表现大幅下降，宏平均F1分数从基准的0.72降低至0.52。
   2. 优化数据与算法：尝试了两种主流方法来解决数据不均衡问题，但效果均不理想：
       * 使用SMOTE技术进行数据增强，导致模型严重过拟合，宏平均F1分数骤降至0.31。
       * 改用Focal Loss损失函数，模型性能完全崩溃，宏平均F1分数仅为0.09。
   3. 尝试混合专家（MoE）架构：这是唯一获得部分成功的尝试。虽然总体准确率从90.8%下降到87.2%，宏平均F1分数从0.72提升到了0.78

这些优化尝试都没有超过基准模型的表现
Deep-Packet作为基准感觉是一个比较简单的模型，但是它在加密流量分析上表现很好，准确率能有96%以上，在开放集识别、增量识别上表现也很好，并且加了好几种间接层都会降低它的表现
这个基准模型的架构是一个基于`ResNet1d`的深度残差网络。其核心是一个包含16个残差块（`BasicBlock`）的1D卷积网络，每个残差块有两层卷积。在残差网络之后，它还连接了三个全连接层，用于最终的分类决策

### 对当前进展的反思
- 数据集是什么样子？：目前用的是ISCX VPN-nonVPN 2016数据集，数据预处理直接用的Deep-Packet提供的方法
- 提取了哪些特征？贡献度如何：
    - 它处理数据的时候没有提取特征，直接把网络包从IP头开始的前1500个字节转化成固定长度为1500的向量了
    - 那可能不行，他们的方法，数据和特征，肯定是他们的比较优。这个策略是不对的，第一步需要先构造专用场景的数据集，不能在他那上面硬做了
    - 把 IP 头和端口混淆之后试下？随机化之后
- 我也做了一些处理，比如我把数据集一个类（Facebook）之外其他类的样本数量都降低到1/10，构建的倾斜数据集上表现还是基准模型更好
    - 这种处理是对的
- 降低到 1/10 之后，每个类是多大规模：
    - 它这个数据集本身类不均衡，大致上top3的类几百万，少数类几千这样
    - 几千还是太多了，现在 few-shot 场景很极端，几十这样
- 这些数据包本身有没有明文字节特征
    - 目前用的数据集有明文特征
    - deep packet 这种流量编码方式非常适合提取明文特征，难怪（基准模型的分类效果那么好）

### 下一步研究计划
- 寻找不带有明文信息的数据集、或者自行编写数据预处理脚本，将数据集中的IP 头和端口作混淆处理
- 在不带有明文信息的数据集上进行训练，获取到Deep-Packet真实的加密流量分类表现
- 验证MoE框架的分类表现，并修改框架使其能更灵活的改变架构和适应分类任务（例如增加专家以实现增量分类）

## 日期：2025年10月25日
### 核心任务：数据集净化与预处理流程强化

本次工作的核心目标是解决“下一步研究计划”中提出的数据集可能存在明文信息泄露的问题。我们通过一系列分析、编码与调试，强化了数据预处理流程，以确保模型训练于更“纯净”的数据之上。

#### 1. 数据污染分析

根据研究计划，我们首先需要确认数据集中是否存在信息泄露。我们以`NonVPN-PCAPs-01/aim_chat_3a.pcap`文件作为样本进行了分析。

- **初步分析**：我们使用`tcpdump`工具检查了数据包内容。分析确认了IP地址和端口号是明文的，这可能导致模型学习到错误的“捷径”。
- **深入分析**：在用户的引导下，我们进一步分析了载荷（payload）部分。我们发现，除了IP和端口，数据集中还存在大量基础设施类协议的明文包，如`ARP`, `DNS`, `NTP`, `NBNS`, `LLMNR`等。这些协议与目标应用流量无关，但具有极强的模式特征，会对模型造成严重干扰。

#### 2. 预处理脚本强化

在明确了污染源后，我们对预处理流程进行了一系列增强，主要集中在`preprocessing.py`和`utils.py`两个文件。

- **IP地址混淆**：通过阅读`preprocessing.py`，我们发现脚本中已有名为`mask_ip`的函数，它会将IP地址置零。这表明IP泄露问题在现有流程中已得到处理。

- **端口混淆**：
    - 我们讨论了多种端口混淆策略。代理最初提出了一个基于确定性随机映射的复杂方案。
    - **用户决策**：用户提出一个更简单、更彻底的“归零”方案。我们采纳了这个建议。
    - **实现**：我们为`preprocessing.py`添加了`mask_ports_to_zero`函数，并增加`--mask-ports`命令行标志来控制此功能的开关。

- **基础设施协议过滤**：
    - 这是本次净化工作的关键一步。我们统一了意见，认为应从数据集中彻底剔除`ARP`, `DNS`, `NTP`, `NBNS`, `LLMNR`这些无关协议。
    - **实现**：我们将所有过滤逻辑集中到了`utils.py`的`should_omit_packet`函数中。通过扩展此函数，我们现在可以有效地在预处理的最早阶段就丢弃这些“噪声”数据包。

#### 3. 数据集结构探索

在处理数据泄露问题的过程中，我们对数据集的内部结构进行了对比分析。

- **对比样本**：我们检查了`vpn_aim_chat1b.pcap` (VPN流量) 和 `AUDIO_spotifygateway.pcap` (Tor流量)。
- **结论**：分析证实，VPN和Tor的pcap文件远比非VPN的“纯净”。由于VPN/Tor的隧道封装机制，上述所有基础设施“噪声”协议均不可见。这一发现为后续实验的数据选择提供了重要参考：**优先使用VPN和Tor数据集可以规避大部分数据污染问题**。

#### 4. 训练流程调试

在用户完成了数据预处理后，我们进入了创建训练/测试集的步骤，但遇到了`ValueError: max() arg is an empty sequence`的报错。

- **问题定位**：通过检查`create_train_test_set.py`的日志和代码，我们发现错误是由于Spark未能读取到任何数据，导致标签列表为空。
- **原因分析**：根本原因在于该脚本默认的`json`读取路径模式 (`/*.json.gz`) 过于简单，无法匹配`preprocessing.py`生成的分块文件名（如 `..._part_0000.json.gz`）。
- **解决方案**：我们通过将文件匹配模式修改为更健壮的递归模式 (`/**/*.json.gz`)，解决了这个问题，使得Spark能够正确发现并加载所有数据分块。

#### 总结

经过本次协作，我们：
1.  **确认并解决了**数据集中存在的多类明文信息泄露问题。
2.  **强化了**数据预处理脚本，使其能够过滤无关协议并提供端口混淆选项。
3.  **明确了**不同子数据集（VPN/Tor vs Non-VPN）的纯净度差异。
4.  **修复了**数据加载脚本中的一个关键Bug，打通了从预处理到训练的数据流程。

当前，数据净化和流程调试工作已完成，可以正式进入模型训练阶段。

## 日期：2025年10月25日
### 核心任务：打通并完善“预处理-训练-评估”全流程

在昨日完成数据净化方案的基础上，今日我们致力于打通整个实验流程，并修复了流程中遇到的多个关键Bug。

#### 1. 调试 `create_train_test_set.py` 脚本

在运行生成训练/测试集的脚本时，我们遇到了两个连续的错误，并逐一解决。

- **错误一：`ValueError: max() arg is an empty sequence`**
    - **定位**：经查，该错误是由于Spark未能从指定的`processed_data/vpn`目录中加载任何`.json.gz`文件，导致标签列表为空。
    - **修复**：我们发现脚本中加载文件的路径模式`/*.json.gz`无法匹配`preprocessing.py`生成的分块文件名。通过将其修改为递归的、更健壮的`/**/*.json.gz`模式，成功解决了文件加载问题。

- **错误二：`ValueError: cannot convert float NaN to integer`**
    - **定位**：此错误在使用`--fraction 0.01`进行小比例采样时出现。原因是采样后部分类别的样本仅剩1个，该样本被分配到测试集后，导致训练集中该类别为空，后续的欠采样逻辑在计算空列表的最小值时出错。
    - **修复**：我们在`split_train_test`函数中增加了安全检查，在执行欠采样逻辑前，判断训练集是否为空。如果为空，则跳过欠采样，从而修复了该错误。

#### 2. 完善 `create_train_test_set.py` 的易用性

在调试过程中，用户反馈`--experiment_type`参数的命名与其实际行为不符，易造成混淆。例如`exp2`会默认执行欠采样，使其训练集变为均衡，这一点从参数名上完全看不出来。

- **初步方案**：代理提议进行一次彻底的重构，废弃`--experiment_type`，将其拆分为`--balance-train`等一系列原子化的命令行参数。
- **最终决策**：用户经过考虑，认为重构风险较大，提出了一个更稳妥的方案：保留现有结构，但增加一个新的实验类型，并完善文档。
- **实现**：
    1.  **新增实验类型**：我们添加了`--experiment_type imbalanced`选项。此选项会执行标准的训练/测试集分割，但**不进行任何欠采样**，完全保留数据的原始分布，满足了用户的核心需求。
    2.  **完善文档**：我们在脚本文件顶部增加了一段详尽的注释，详细说明了每一种`experiment_type`（包括`exp2`会使训练集均衡，`exp3`会使全局数据均衡等）的具体行为和适用场景，彻底解决了参数不明确的问题。

#### 3. 修复训练与评估脚本

在准备好数据并开始训练后，我们发现并修复了后续脚本中的两个关键Bug。

- **修复 `train_resnet.py`**：用户指出，为`traffic`任务调用`train_traffic_classification_resnet_model`函数时，`validation_split`参数没有被正确处理。经检查`ml/utils.py`，发现该函数确实遗漏了此参数的接收和传递。我们参照正确的实现，为其补全了逻辑。

- **修复 `evaluation.py`**：在生成最终评估指令时，通过阅读代码发现，该脚本对`--model_type resnet`的处理逻辑存在`NotImplementedError`，即作者并未完成标准ResNet模型的加载功能。
    - **实现**：我们参照`ml/utils.utils`中的加载逻辑，为`evaluation.py`补全了加载`ResNet`模型的代码，使其能够正常工作。

#### 总结

经过今日的密集协作，我们成功打通了从数据生成到模型评估的完整链路。通过修复多个脚本中的关键Bug并极大地改善了数据生成脚本的清晰度和易用性，现在整个实验流程已准备就绪，可以随时开始正式的模型训练和评估。

## 日期：2025年11月1日

### 核心任务：设计并实现简化的混合专家（MoE）系统，以提升对少数类的识别能力

#### 1. 确定少数类

*   **目标**：从新的VPN流量数据集中识别出基准模型表现不佳的少数类。
*   **过程**：
    *   创建 `check_labels.py` 脚本，用于分析 `processed_data/vpn` 目录下预处理数据的 `traffic_label` 分布。
    *   解决了 `pyspark` 环境配置、`psutil` 依赖缺失、`conda run` 管道问题以及文件读取方式等一系列技术障碍。
    *   分析发现 `app_label` 字段在VPN数据中为空，因此将分析重点转向 `traffic_label`。
*   **结论**：根据 `traffic_label` 的分布统计，确定标签 `5` ("VPN: Chat") 和 `7` ("VPN: Email") 为少数类。

#### 2. 构建少数类专家数据集

*   **目标**：为少数类专家模型准备专门的训练/测试数据集。
*   **过程**：
    *   重构 `create_train_test_set.py` 脚本，使其可以通过 `--minority-classes` 命令行参数动态指定少数类标签，增强了脚本的灵活性。
    *   成功运行修改后的脚本，在 `train_test_data/traffic_minority_expert` 目录下生成了包含类别5和7的数据集。

#### 3. 融合策略的探索与验证

*   **用户初始设想**：通过基准模型的预测置信度来判断是否“模糊”，从而决定是否将样本路由给少数类专家。
*   **验证过程**：
    *   创建 `analyze_baseline_confidence.py` 脚本，用于分析基准模型在少数类样本上的预测置信度。
    *   解决了脚本的参数传递问题（从stdin改为命令行参数）。
    *   运行分析脚本，得到关键发现。
*   **验证结果**：分析显示，基准模型在错误分类少数类样本时，平均置信度约为60.63%。这表明模型并非“不确定”，而是“自信地犯错”。因此，简单依赖置信度阈值来判断“模糊”并触发专家模型的方案，效果可能不佳。

#### 4. 确定并实施模型融合（Ensemble）方案

*   **策略调整**：鉴于基准模型“自信地犯错”的特性，我们放弃了基于置信度阈值的路由方案，并排除了基于预测标签的路由方案（因为基准模型从不预测少数类）。
*   **最终选择**：决定采用**模型融合（Ensemble）**方案，即同时运行基准模型和少数类专家模型，并通过加权平均融合它们的预测概率。
*   **实现**：
    *   对 `evaluation.py` 脚本进行了大规模重构，增加了 `--eval-mode` 参数，支持 `standard` 和 `ensemble` 两种评估模式。
    *   在 `ensemble` 模式下，脚本能够加载基准模型和少数类专家模型，正确处理少数类标签的映射，并实现概率的加权融合。

#### 总结

今天的工作成功地识别了新数据集中的少数类，构建了相应的专家数据集，并通过实证分析验证了基准模型在少数类上的行为。基于这些发现，我们最终确定并实现了模型融合的评估框架。下一步将是使用这个框架对融合模型进行实际评估。

## 日期：2025年10月31日
基准模型表现：
  --- Evaluation Results ---
  Accuracy: 0.9633

  Classification Report:
                precision    recall  f1-score   support

             5       0.00      0.00      0.00       215
             6       0.75      0.91      0.82      1034
             7       0.00      0.00      0.00        65
             8       1.00      0.99      1.00      4408
             9       0.99      0.98      0.98      1089
            10       0.97      0.98      0.98      9476

      accuracy                           0.96     16287
     macro avg       0.62      0.64      0.63     16287
  weighted avg       0.95      0.96      0.96     16287
  minority_weight = 0.15时的表现：
  --- Evaluation Results ---
  Accuracy: 0.9641

  Classification Report:
                precision    recall  f1-score   support

             5       0.29      0.07      0.11       215
             6       0.76      0.90      0.82      1034
             7       0.71      0.08      0.14        65
             8       1.00      0.99      1.00      4408
             9       0.99      0.98      0.98      1089
            10       0.98      0.98      0.98      9476

      accuracy                           0.96     16287
     macro avg       0.79      0.67      0.67     16287
  weighted avg       0.96      0.96      0.96     16287
  minority_weight=0.1时
  --- Evaluation Results ---
  Accuracy: 0.9634

  Classification Report:
                precision    recall  f1-score   support

             5       0.20      0.01      0.02       215
             6       0.75      0.90      0.82      1034
             7       0.67      0.03      0.06        65
             8       1.00      0.99      1.00      4408
             9       0.99      0.98      0.98      1089
            10       0.97      0.98      0.98      9476

      accuracy                           0.96     16287
     macro avg       0.76      0.65      0.64     16287
  weighted avg       0.96      0.96      0.96     16287
  minority_weight=0.2时
  --- Evaluation Results ---
  Accuracy: 0.9592

  Classification Report:
                precision    recall  f1-score   support

             5       0.19      0.23      0.21       215
             6       0.77      0.88      0.82      1034
             7       0.86      0.18      0.30        65
             8       1.00      0.99      1.00      4408
             9       0.99      0.98      0.98      1089
            10       0.98      0.97      0.98      9476

      accuracy                           0.96     16287
     macro avg       0.80      0.71      0.71     16287
  weighted avg       0.96      0.96      0.96     16287

教师对下一步的指导：

## 日期：2025年11月7日
### 核心任务：构建可学习的融合策略，并明确研究框架与目标

今天的工作围绕着将“简单加权融合”升级为“可学习的智能融合”，并在此过程中，将具体的技术工作与宏观的研究目标进行对齐，形成了一套清晰的研究框架。

#### 1. 门控网络的设计与实现

*   **初始设计**：我们设计了一个名为`GatingNetwork`的MLP（多层感知机），它接收基准模型和专家模型的概率输出，通过一个可学习的非线性变换，来取代固定的加权平均。
*   **重大失误与修正**：
    *   **失误**：在最初的实现中，我犯了一个严重的方法论错误，即在`evaluation.py`脚本中，直接使用了**测试集**来训练门控网络。这会导致数据泄露，评估结果虚高且无效。
    *   **修正**：在用户的指正下，我们立刻修正了该流程。我们将训练和评估彻底分离：
        1.  新建了 `train_gating_network.py` 脚本，其唯一职责是在**训练集**上学习门控网络的参数。
        2.  重构了 `evaluation.py` 脚本，使其在`gating_ensemble`模式下，只加载**预训练好**的门控网络，进行纯粹的评估。
*   **代码清理**：根据用户指令，我们移除了项目中与之前失败的MoE（Mixture-of-Experts）尝试相关的所有代码，包括`train_moe.py`以及其他脚本中的引用，保持了代码库的整洁。

#### 2. 实验结果分析：为何门控网络表现不佳

在运行了（错误的）初步实验后，我们观察到一个关键现象：门控网络虽然提升了整体准确率，但在核心的少数类识别上，表现退化到了与基准模型相同的零分。

*   **核心结论**：门控网络为了追求更高的**整体准确率**，学会了**完全忽略**“少数类专家”的意见。
*   **根本原因**：门控网络的训练过程受到了**极端类别不均衡**的误导。标准的交叉熵损失函数被数量庞大的多数类样本所主导，使得网络发现“永远相信基准模型”是最小化总损失的捷径。
*   **重要启示**：这个结果反向证明了“简单加权”策略的价值所在——它通过一个“强制规定”，保证了少数类专家的意见总能被听到。这也揭示了我们下一步工作的方向：必须在训练门控网络时，解决类别不均衡问题。

#### 3. 明确研究框架、目标与评估方案

今天最有价值的结论，是我们深入讨论并明确了整个研究的顶层设计。

*   **研究贡献定义**：我们将当前的工作明确为构建一个名为 **“基于门控专家集成的增量式识别模型 (Gated Expert Ensemble, GEE)”** 的技术框架。

*   **评价指标体系 (YYY)**：我们确定了一套全面的指标来衡量模型效果：
    1.  **增量学习效果**：**宏平均F1分数 (Macro-F1)** 为核心指标，辅以新增类别的F1分数。
    2.  **开放集识别效果**：**AUROC** 为核心指标，辅以准确率-拒绝率曲线。
    3.  **增量效率**：训练时间、新增参数等。

*   **核心对比基准**：我们确立了三个必须进行比较的基准：
    1.  **原始基准模型**：证明我们的框架优于“不学习”。
    2.  **全量微调模型**：证明我们的框架在性能接近或更优的同时，效率远超传统方法。
    3.  **简单加权集成**：证明我们“可学习的门控”优于“固定的规则”。

*   **澄清开放集对比方法**：我们明确了如何与一个本身不支持开放集的Deep-Packet模型进行对比。方法是为Deep-Packet模型增加一个**“基于Softmax置信度阈值”**的后处理决策层，使其具备开放集识别能力，然后将其作为基准。我们要证明的是，我们的GEE模型能为这个决策层提供更优质、更可靠的置信度信号。

#### 下一步计划

根据今天的分析，下一步的工作非常明确：
*   **为 `train_gating_network.py` 脚本以 Macro-F1 + Accuracy 联合指标 作为训练目标。**

## 日期：2025年11月9日
### 核心任务：实现Macro-F1联合优化的门控网络训练系统

#### 1. 新增文件

- **config/gating_network_test.yaml**：完整测试配置
- **config/gating_network_quick_test.yaml**：快速测试配置
- **config/README.md**：使用说明文档
- **ml/losses.py**：Macro-F1损失函数实现
- **test_macro_f1_loss.py**：损失函数测试脚本

#### 2. 修改文件

**train_gating_network.py**：
- 新增Macro-F1相关命令行参数
- 支持联合损失训练，实时显示损失分解
- 日志输出改为"Macro-F1 Loss"明确标识

**test_gating_networks.py**：
- 从命令行参数改为YAML配置文件驱动
- 修复评估脚本参数格式问题
- 优化日志输出，保留换行符
- 实现自动化性能排名和报告生成

**evaluation.py调用**：
- 修复参数格式：`--data_path`、`--gating_network_path`、`--eval-mode`
- 修复`--minority_classes`参数格式问题

#### 3. 实验结果发现

**关键发现**：Macro-F1联合优化大幅降低整体表现，无法识别到少数类, 因为F1-Score是离散的，无法被梯度更新利用，模型无法从中学习到结果

#### 下一步工作
- 在训练门控网络时，使用“加权交叉熵损失 (Weighted Cross-Entropy Loss)”，增大少数类样本的权重
- 执行数据重采样，增加少数类样本出现的概率
- 继续调研数据增强方案
- 再调研几个MoE门控网络结构，比较简单的网络（图像识别，AAAI）


## 日期：2025年11月16日
今日主题：
- 在训练门控网络时，使用“加权交叉熵损失 (Weighted Cross-Entropy Loss)”，增大少数类样本的权重
- 执行数据重采样，增加少数类样本出现的概率
- 继续调研数据增强方案
- 再调研几个MoE门控网络结构，比较简单的网络（图像识别，AAAI）

### 任务1: 使用“加权交叉熵损失 (Weighted Cross-Entropy Loss)”，验证门控网络的表现
 **实现方式**：
 1.  **修改 `train_gating_network.py`**：在训练流程中，首先根据训练集（`train_dataset`）的标签分布，为每个类别计算权重。权重计算采用了 `n_samples / (n_classes * n_samples_per_class)` 的策略，可以有效放大少数类在损失计算中的影响。
 2.  **应用权重**：将计算出的 `class_weights` 张量传递给损失函数 `criterion = torch.nn.CrossEntropyLoss(weight=class_weights)`。
 3.  **修正 `train_labels` 计算**：修复了原先通过遍历`dataloader`来获取标签的低效且不严谨的方式，改为从`Subset`对象中直接索引获取，`train_labels = train_dataset.dataset.tensors[1][train_dataset.indices]`，提高了效率和准确性。

    用简单的加权策略、minority_weight = 0.15时的表现：
    --- Evaluation Results ---
    Accuracy: 0.9641

    Classification Report:
                    precision    recall  f1-score   support

                5       0.29      0.07      0.11       215
                6       0.76      0.90      0.82      1034
                7       0.71      0.08      0.14        65
                8       1.00      0.99      1.00      4408
                9       0.99      0.98      0.98      1089
                10       0.98      0.98      0.98      9476

        accuracy                           0.96     16287
        macro avg       0.79      0.67      0.67     16287
    weighted avg       0.96      0.96      0.96     16287

    用门控网络结合两个模型的输出
    --- Evaluation Results ---
    Accuracy: 0.9500

    Classification Report:
                precision    recall  f1-score   support

            5       0.18      0.60      0.28       215
            6       0.83      0.80      0.82      1034
            7       0.84      1.00      0.92        65
            8       1.00      0.99      1.00      4408
            9       1.00      0.98      0.99      1089
            10       0.99      0.95      0.97      9476

        accuracy                           0.95     16287
    macro avg       0.81      0.89      0.83     16287
    weighted avg       0.97      0.95      0.96     16287

    基准模型的输出
    --- Evaluation Results ---
    Accuracy: 0.9633

    Classification Report:
                    precision    recall  f1-score   support

                5       0.00      0.00      0.00       215
                6       0.75      0.91      0.82      1034
                7       0.00      0.00      0.00        65
                8       1.00      0.99      1.00      4408
                9       0.99      0.98      0.98      1089
                10       0.97      0.98      0.98      9476

        accuracy                           0.96     16287
        macro avg       0.62      0.64      0.63     16287
    weighted avg       0.95      0.96      0.96     16287、


## 日期：2025年11月22日
  
   ### 当前研究目标
   本次研究的核心目标是，对一个基线ResNet模型进行严格的开放集识别（Open-Set
   Recognition）性能评估。我们采用6折交叉验证的方法，轮流将一个类别作为“未知类”并从训练集中排除，以此来测试模型对从未见过的数据的识别能力。评估的核心指标是 **AUROC** 和 **FPR@TPR95**。
  
   这个评估将为我们后续研究更复杂的模型（如Gated Expert Ensemble, GEE）提供一个坚实的性能基准。
  
   ### 已取得的研究进展
   1.  **确定了评估流程**: 我们建立了一套严格的开放集测试流程，确保在每一折的验证中，模型都完全没有接触过“未知类”的数据。
   2.  **自动化脚本开发**:
       *   开发并成功调试了 `scripts/open_set_test_1116.sh` 脚本，实现了6折交叉验证中“数据生成”和“模型训练”两个阶段的完全自动化。
       *   开发并成功调试了 `scripts/run_evaluation_1116.sh` 脚本，实现了对6个已训练模型的“自动化评估”。
   3.  **关键代码模块修复与增强**:
       *   **`create_train_test_set.py`**: 已能根据需要，在生成训练集时排除特定的“未知类”。
       *   **`train_resnet.py`**: 已支持动态传入输出维度，使其能够适应不同折（folds）中数量变化的“已知类”。
       *   **`evaluation.py`**: 这是进展中最关键也最艰难的一步。经过多次迭代和修复（特别是解决了由于标签映射错误导致的分类指标恒为0的问题），该脚本现在功能完备：
           *   能够正确处理开放集评估所需的各类参数（`--open-set-eval`, `--known-classes`, `--label-map` 等）。
           *   通过 `--label-map` 参数，成功将模型内部的预测标签（如 0, 1, 2）映射回真实的类别标签（如 6, 7, 8），从而计算出有意义的分类指标。
           *   能够同时输出分类指标（准确率、F1等）和开放集指标（AUROC, FPR@TPR95）。
   4.  **完成基线模型评估**: 我们已经成功运行了完整的6折交叉验证评估流程，并获得了基线ResNet模型在开放集场景下的各项性能指标。所有结果均已正确生成并记录在日志文件中。
  
   ### 下一步计划
   1.  **深入分析基线结果**: 仔细分析已获得的基线模型评估结果，解读其在不同类别作为未知类时的性能表现差异，总结模型的性能瓶颈和优势。
   2.  **实现Gated Expert Ensemble (GEE)模型**: 现在基线已经建立，下一步的核心工作是实现并训练GEE模型。
   3.  **对比实验**: 使用相同的6折交叉验证框架，对GEE模型进行开放集性能评估。
   4.  **总结与结论**: 将GEE模型的评估结果与基线ResNet模型进行全面对比，以判断GEE架构是否在本项目的数据集和场景下，对开放集识别性能有显著提升。

### 基线模型开放集性能评估分析

我们对 `.local/logs/1122.txt` 文件中的6折交叉验证结果进行了深入分析，得出以下核心结论与问题：

#### 核心结论

1.  **性能高度不稳定**: 模型的开放集识别能力极不稳定，其表现完全取决于哪个类别被当作“未知类”。当排除小众且特征独特的类别（如类别6和7）时，模型表现优秀（AUROC > 0.97）；但在其他情况下则遭遇彻底失败。
2.  **多数类导致灾难性失败**: 该基线模型对于未见过的大规模流量类别不具备鲁棒性。当一个多数类（如类别8或10）被作为未知类时，模型的整体准确率会崩溃，并且模型会以高置信度将这些未知流量错误地归类为某个已知类别，而不是将它们识别为“新类别”。
3.  **类别6成为“黑洞”**: 在失败的测试中（即排除类别8或10时），模型压倒性地将海量的未知样本误判为类别6。这表明模型为类别6学习到的特征过于泛化，使其成为一个无法识别数据的““黑洞””，吸收了大量未知流量。
4.  **聚合指标具有误导性**: 报告中0.94的平均AUROC具有欺骗性，因为它掩盖了各折之间巨大的性能差异。一个更能说明问题的指标——**平均FPR@TPR95**（在95%真阳性率下的假阳性率）——高达**0.4756**。这个数字意味着，为了在已知类别上达到95%的召回率，平均有将近一半的未知流量会被错误地识别为“已知”，这是一个非常糟糕的结果。

#### 暴露的根本问题

1.  **开放集识别存在严重缺陷**: 最核心的问题是模型完全无法处理作为未知项的多数类。它没有将这些流量识别为“新颖的”，而是以高置信度将其强制归入一个已知的类别。这与开放集识别系统的根本目标背道而驰，代表了当前基线方法的严重失败。
2.  **新颖性检测的泛化能力差**: 模型没有学到一个能够有效区分“分布内”和“分布外”数据的特征空间。当类别8、9和10作为未知类时，其极高的FPR@TPR95值（分别为0.97、0.83和0.67）清楚地证明了这一点。
3.  **基线方法能力不足**: 本次评估明确证明，一个标准的ResNet模型，仅仅依靠Softmax置信度作为阈值，不是一个适用于当前场景的可行的开放集识别解决方案。

这个分析结论强有力地证明了 `instruction.md` 中规划的研究方向的正确性：必须采用更先进的策略。基线模型的失败，也反过来凸显了后续探索**混合专家（MoE）或门控专家集成（GEE）**架构作为高级新颖性信号来源的巨大潜力和必要性。
## 日期：2025年11月29日

### 核心任务：问题回溯与重大失误修正

#### 1. 问题现象
在执行GEE开放集评估脚本 (`run_gee_open_set_evaluation.sh`) 的过程中，我们遭遇了一系列看似无关的系统级错误，包括：
*   Spark在写入数据时，因无法清理目录而失败 (`IOException`)。
*   Spark因内存溢出（OOM）导致 `SparkContext` 意外关闭。
*   `train_resnet.py` 脚本在加载数据时，因内存溢出被操作系统强行杀死 (`Killed`)。

#### 2. 根本原因定位：一次严重的疏忽
经过用户的提醒和代码比对，我们定位到所有这些问题的**唯一根源**：我在创建 `run_gee_open_set_evaluation.sh` 脚本时，**遗漏了关键的数据集采样参数**。

*   **失误详情**：在调用 `create_train_test_set.py` 脚本时，我未能从基准测试脚本 (`open_set_test_1116.sh`) 中继承 `--fraction 0.01` 和 `--batch_size 50` 这两个参数。
*   **直接后果**：这导致数据生成脚本处理了**100%的完整数据集**，而不是预期的**1%的采样数据**。数据量从可控范围暴增了100倍。
*   **连锁反应**：巨大的数据量压垮了系统的内存，从而引发了上述所有OOM相关的错误。我们后续所有针对Spark内存配置、PyTorch数据加载方式的调试和修复，都是在对一个错误的前提（处理全量数据）进行“修复”，治标不治本。

#### 3. 影响与反思
这是一次非常重大的失误。它直接导致了：
1.  过去一段时间内所有针对OOM错误的调试都偏离了方向。
2.  所有已生成的GEE实验数据集 (`train_test_data/open_set_gee/`) 均是错误的（基于全量数据），必须被废弃。
3.  所有基于这些错误数据集训练的模型 (`model/open_set_gee/`) 也完全无效，必须被删除。
4.  整个GEE开放集识别实验的工作必须**从头开始**。

我对此表示深刻的反思。在组合和创建新的实验流程时，必须对每一个环节，特别是数据源头部分的参数进行严格的交叉比对，而不能只关注于新增的逻辑。

#### 4. 下一步计划
1.  **重启实验**：从零开始，重新执行GEE开放集评估实验。